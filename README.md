# üõ°Ô∏è AI Exam Proctoring System

A **production-ready, ultra-lightweight** AI-powered exam proctoring system that monitors students in real-time using face recognition, object detection, and voice activity detection ‚Äî all running on CPU with **< 10 MB total model size** via WebSocket.

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-009688?logo=fastapi&logoColor=white)
![React](https://img.shields.io/badge/React-18+-61DAFB?logo=react&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-5C3EE8?logo=opencv&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green)

---

## ‚ú® Features

| Feature | Description |
|---------|-------------|
| **üîê Face Registration** | Captures user face via webcam, extracts a 512-D embedding, stores in memory |
| **üë§ Identity Verification** | Continuously matches live face against the registered reference using cosine similarity |
| **üë• Multiple Face Detection** | Detects if more than one person is in the frame |
| **üö´ No Face Detection** | Flags when the student leaves the camera view |
| **üìµ Forbidden Object Detection** | Scans frames for cell phones, laptops, monitors/tablets, and watches/clocks |
| **üì∑ Camera Block Detection** | Detects when the webcam is covered or turned off (brightness + variance analysis) |
| **üé§ Voice Activity Detection** | Detects speech during the exam using Silero VAD |
| **‚ö° Real-time WebSocket** | Self-pacing WebSocket loop for low-latency, real-time AI analysis |
| **üü¢üü°üî¥ Status UI** | Dynamic border colors ‚Äî Green (OK), Yellow (Warning), Red (Flagged) |
| **üìä Live Dashboard** | Shows identity, face count, similarity score, objects, and audio status |
| **üö© Flag History** | Logs every violation with timestamps |

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      REACT FRONTEND                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Webcam   ‚îÇ‚Üí ‚îÇ Canvas    ‚îÇ‚Üí ‚îÇ Base64 JPEG ‚Üí WebSocket   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Stream   ‚îÇ  ‚îÇ Capture   ‚îÇ  ‚îÇ Self-pacing loop    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ Audio    ‚îÇ‚Üí WAV encode ‚Üí base64 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                 ‚îÇ
‚îÇ  ‚îÇ Capture  ‚îÇ  (every 3 seconds)           ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ  ‚îÇ Live Analysis: Identity, Faces, Similarity, Objects, Audio‚îÇ
‚îÇ  ‚îÇ Flag History: Timestamped violation log                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ WebSocket (ws:// or wss://)
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     FASTAPI BACKEND                          ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  POST /exam/start     ‚Üí Register face embedding (HTTP)       ‚îÇ
‚îÇ  WS   /exam/ws/{id}   ‚Üí Real-time frame + audio analysis     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  ProctorEngine   ‚îÇ  ‚îÇ ObjectDetector ‚îÇ  ‚îÇ AudioDetector‚îÇ ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                ‚îÇ  ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ 1. Brightness    ‚îÇ  ‚îÇ YOLOv8n INT8   ‚îÇ  ‚îÇ Silero VAD   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ 2. YuNet face    ‚îÇ  ‚îÇ 640√ó640 input  ‚îÇ  ‚îÇ 16kHz mono   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ 3. Align 112√ó112 ‚îÇ  ‚îÇ NMS + filter   ‚îÇ  ‚îÇ Speech prob  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ 4. MobileFaceNet ‚îÇ  ‚îÇ Forbidden only ‚îÇ  ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ 5. Cosine sim    ‚îÇ  ‚îÇ                ‚îÇ  ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ü§ñ AI Models

All models are stored in the `models/` directory (~9.1 MB total).

| Model | File | Task | Size |
|-------|------|------|------|
| **YuNet** | `face_detection_yunet_2023mar.onnx` | Face detection (bbox + 5 landmarks) | **0.22 MB** |
| **MobileFaceNet INT8** | `mobilefacenet_int8.onnx` | Face recognition (512-D embedding) | **3.35 MB** |
| **YOLOv8n INT8** | `yolov8n_int8.onnx` | Object detection (filtered to 4 classes) | **3.34 MB** |
| **Silero VAD** | `silero_vad.onnx` | Voice activity detection | **2.2 MB** |
| | | **Total** | **~9.1 MB** |

---

## üìÅ Project Structure

```
d:\face\
‚îú‚îÄ‚îÄ main.py                 # FastAPI server (HTTP + WebSocket endpoints)
‚îú‚îÄ‚îÄ engine.py               # ProctorEngine (face detection + recognition)
‚îú‚îÄ‚îÄ object_detector.py      # ObjectDetector (YOLOv8n forbidden objects)
‚îú‚îÄ‚îÄ vad_engine.py           # AudioDetector (Silero VAD speech detection)
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ render.yaml             # Render deployment config
‚îú‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ face_detection_yunet_2023mar.onnx   (0.22 MB)
‚îÇ   ‚îú‚îÄ‚îÄ mobilefacenet_int8.onnx             (3.35 MB)
‚îÇ   ‚îú‚îÄ‚îÄ yolov8n_int8.onnx                   (3.34 MB)
‚îÇ   ‚îî‚îÄ‚îÄ silero_vad.onnx                     (2.2 MB)
‚îÇ
‚îî‚îÄ‚îÄ frontend/               # React + Vite
    ‚îú‚îÄ‚îÄ package.json
    ‚îú‚îÄ‚îÄ vite.config.js      # Dev proxy + WebSocket config
    ‚îú‚îÄ‚îÄ index.html
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ main.jsx
        ‚îú‚îÄ‚îÄ App.jsx         # Main component (webcam, WebSocket, UI)
        ‚îî‚îÄ‚îÄ App.css         # Dark glassmorphism theme
```

---

## üöÄ Getting Started

### Prerequisites

- **Python 3.10+** ‚Äî [Download](https://www.python.org/downloads/)
- **Node.js 18+** ‚Äî [Download](https://nodejs.org/)
- **Webcam** ‚Äî Built-in or USB camera

### Installation

```bash
# Clone the repository
git clone https://github.com/jagdees2004/Exam-Proctor.git
cd Exam-Proctor

# Install Python dependencies
pip install -r requirements.txt

# Install frontend dependencies
cd frontend
npm install
cd ..
```

### Running the Application

You need **two terminals**:

#### Terminal 1: Backend
```bash
python main.py
```
Output:
```
[ProctorEngine] Initialized with YuNet + MobileFaceNet (onnxruntime)
[ObjectDetector] Loaded YOLOv8n INT8 ONNX model (onnxruntime)
[AudioDetector] Loaded Silero VAD ONNX model
INFO:     Uvicorn running on http://0.0.0.0:8000
```

#### Terminal 2: Frontend
```bash
cd frontend
npm run dev
```

#### Open in Browser

Navigate to **http://localhost:3000**:

1. **Allow camera + microphone** when prompted
2. Enter a **Student ID** (e.g., `student1`)
3. Click **üöÄ Start Exam** ‚Äî face is registered
4. Real-time monitoring starts automatically via WebSocket
5. Click **‚èπ End Exam** to stop

---

## üì° API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/exam/start` | POST | Register face (multipart: `user_id` + `file`) |
| `/exam/ws/{user_id}` | WebSocket | Real-time frame + audio analysis |
| `/exam/verify` | POST | One-shot face verification (legacy) |
| `/exam/objects` | POST | One-shot object detection (legacy) |
| `/exam/audio` | POST | One-shot audio analysis (legacy) |
| `/health` | GET | Health check |

### WebSocket Protocol (`/exam/ws/{user_id}`)

**Send frame:**
```json
{"type": "frame", "data": "data:image/jpeg;base64,..."}
```

**Receive video result:**
```json
{
  "type": "video_result",
  "identity_match": true,
  "face_count": 1,
  "similarity_score": 0.7823,
  "status": "ok",
  "forbidden_objects": [],
  "flagged": false
}
```

**Send audio:**
```json
{"type": "audio", "data": "<base64 WAV>"}
```

**Receive audio result:**
```json
{
  "type": "audio_result",
  "is_talking": false,
  "speech_prob": 0.03,
  "flagged": false
}
```

**Status values:** `ok`, `no_face`, `multiple_faces`, `identity_mismatch`, `camera_blocked`, `not_registered`, `error`

---

## üöÄ Deployment

### Backend (Render)

The project includes `render.yaml` for one-click deployment to [Render](https://render.com):

```yaml
services:
  - type: web
    name: proctor-backend
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn main:app --host 0.0.0.0 --port $PORT
```

### Frontend (Vercel / Netlify)

Deploy the `frontend/` directory. Set the environment variable:

```
VITE_API_URL=https://your-render-backend.onrender.com
```

---

## ‚öôÔ∏è Configuration & Thresholds

### `engine.py`
| Constant | Default | Purpose |
|----------|---------|---------|
| `FACE_SCORE_THRESHOLD` | `0.6` | YuNet face detection confidence |
| `COSINE_SIMILARITY_THRESHOLD` | `0.30` | Minimum similarity for identity match |
| `BRIGHTNESS_THRESHOLD` | `40` | Mean pixel brightness below this ‚Üí camera blocked |
| `VARIANCE_THRESHOLD` | `15` | Pixel variance below this ‚Üí camera covered |

### `object_detector.py`
| Constant | Default | Purpose |
|----------|---------|---------|
| `CONFIDENCE_THRESHOLD` | `0.2` | YOLO detection confidence |
| `NMS_THRESHOLD` | `0.45` | Non-Maximum Suppression |
| `INPUT_SIZE` | `640` | Fixed ONNX input size (do not change) |

### `vad_engine.py`
| Constant | Default | Purpose |
|----------|---------|---------|
| `SPEECH_THRESHOLD` | `0.5` | Probability above this ‚Üí speech detected |
| `TARGET_SAMPLE_RATE` | `16000` | Required by Silero VAD |

---

## üêõ Troubleshooting

| Problem | Solution |
|---------|----------|
| Camera access denied | Allow permissions in browser settings ‚Üí reload |
| WebSocket not connecting | Ensure `vite.config.js` has `ws: true` in the proxy |
| `Got: 320 Expected: 640` | Never change `INPUT_SIZE` ‚Äî the INT8 model has fixed shape |
| Server port in use | Kill process: `netstat -ano \| findstr 8000` then `taskkill /PID <pid> /F` |
| Phone not detected | Lower `CONFIDENCE_THRESHOLD` in `object_detector.py` (try `0.15`) |
| False face matches | Increase `COSINE_SIMILARITY_THRESHOLD` in `engine.py` (try `0.4`) |
| Slow on Render free tier | Expected ‚Äî YOLOv8 at 640√ó640 is CPU-heavy; upgrade to paid plan for speed |

---

## üìÑ License

This project is open-source under the [MIT License](LICENSE).

---

## üôè Acknowledgments

- [OpenCV Zoo](https://github.com/opencv/opencv_zoo) ‚Äî YuNet face detection model
- [MobileFaceNet](https://github.com/nicholaspat/MobileFaceNet_PyTorch) ‚Äî Face recognition model
- [Ultralytics](https://github.com/ultralytics/ultralytics) ‚Äî YOLOv8 model
- [Silero VAD](https://github.com/snakers4/silero-vad) ‚Äî Voice activity detection
- [ONNXRuntime](https://onnxruntime.ai/) ‚Äî Fast model inference and quantization
